{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embrace Randomness - in XGBoost\n",
    "\n",
    "**IMPORTANT:** XGBoost is deterministic when the sampling parameters *subsample* and *colsample_by_** are not changed (i.e. left to the default value of 1.0).\n",
    "\n",
    "Thus, running XGBoost with the default parameters will always return the same model (given the same training set as input). Even changing XGBoost's random state has no effect in this case - because it only comes into play when sampling is used.\n",
    "\n",
    "However, to prevent overfitting it is common to test different values for the sampling parameters (e.g. during hyperparameter optimization). Consequently, XGBoost will generate different models for the same input data when trained repeatedly.\n",
    "\n",
    "There are two ways to deal with the randomness:\n",
    "\n",
    "1. set *random_state* to a fixed value - this is good for reproducability, but **not for production-ready models!** (TODO: add reference)\n",
    "2. train the model many times with cross validation and choose the model with the highest mean score as final model, i.e. the model which achieves the best generalization on the given training and test data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will\n",
    "\n",
    "* Train XGBoost with default parameters (deterministic models).\n",
    "* Train XGBoost with resampling (random models).\n",
    "* Train XGBoost with resampling and cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "* Load dataset (breast cancer - clean data, no missing values, no features engineering necessary)\n",
    "* Split into training and test data (70%/30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "# IMPORTANT: switch target labels as malignant should be 1\n",
    "\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = 1-pd.Series(data['target'], name='target')\n",
    "\n",
    "labels = data['target_names'][[1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  (398, 30) , test: (171, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=random_seed)\n",
    "print(\"train: \", X_train.shape, ', test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic: XGBoost with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rounds = 100   # maximum number of boosting iterations\n",
    "early_stop = 50     # stop if metric does not improve for X rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def train_xgb_sklearn(params, X_train, y_train, X_test, y_test, random_seed):\n",
    "    '''Train and predict with Scikit-Learn XGBClassifier'''\n",
    "    clf = XGBClassifier(n_estimators=max_rounds, **params, random_state=random_seed)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "def train_xgb_native(params, X_train, y_train, X_test, y_test, random_seed):\n",
    "    params = {**params, 'seed':random_seed}\n",
    "    train = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    test  = xgb.DMatrix(X_test.values, y_test.values)\n",
    "    bst = xgb.train(params, train, max_rounds)\n",
    "    return bst.predict(test)\n",
    "\n",
    "def eval_metrics(y_true, y_hat):\n",
    "    return {\n",
    "        'roc': roc_auc_score(y_true, y_hat),\n",
    "        'acc': accuracy_score(y_true, y_hat >= 0.5),\n",
    "        'wrong': y_true[y_true != (y_hat >= 0.5)].index.values,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_n_times(n, xgb_train, params, verbose=True):\n",
    "    print(f\"RUNNING {n}-times '{xgb_train.__name__}' with {params}\")\n",
    "    results=[]\n",
    "    for i in range(n):\n",
    "        random_seed = np.random.randint(1000)\n",
    "        y_hat = xgb_train(params, X_train, y_train, X_test, y_test, random_seed)\n",
    "        metrics = {**eval_metrics(y_test, y_hat), 'seed':random_seed}\n",
    "        results.append(metrics)\n",
    "        if verbose:\n",
    "            print(\"roc_auc={roc:.4f}, accuracy={acc:.4f}, wrong:{wrong}, seed:{seed}\".format(**metrics))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING 5-times 'train_xgb_sklearn' with {'objective': 'binary:logistic'}\n",
      "roc_auc=0.9949, accuracy=0.9649, wrong:[205   5  86 193  73 385], seed:185\n",
      "roc_auc=0.9949, accuracy=0.9649, wrong:[205   5  86 193  73 385], seed:27\n",
      "roc_auc=0.9949, accuracy=0.9649, wrong:[205   5  86 193  73 385], seed:481\n",
      "roc_auc=0.9949, accuracy=0.9649, wrong:[205   5  86 193  73 385], seed:995\n",
      "roc_auc=0.9949, accuracy=0.9649, wrong:[205   5  86 193  73 385], seed:546\n",
      "RUNNING 5-times 'train_xgb_native' with {'objective': 'binary:logistic'}\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[205  86  73], seed:582\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[205  86  73], seed:484\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[205  86  73], seed:795\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[205  86  73], seed:499\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[205  86  73], seed:22\n"
     ]
    }
   ],
   "source": [
    "# using default XGBoost parameters\n",
    "params = {'objective':'binary:logistic'}\n",
    "\n",
    "for fun in [train_xgb_sklearn, train_xgb_native]:\n",
    "    eval_n_times(5, fun, params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random: with subsample parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING 10-times 'train_xgb_sklearn' with {'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "roc_auc=0.9968, accuracy=0.9708, wrong:[205  86 193  73 385], seed:732\n",
      "roc_auc=0.9969, accuracy=0.9766, wrong:[205  86  73 385], seed:139\n",
      "roc_auc=0.9956, accuracy=0.9766, wrong:[ 86 193  73 385], seed:533\n",
      "roc_auc=0.9974, accuracy=0.9766, wrong:[205  86  73 385], seed:384\n",
      "roc_auc=0.9961, accuracy=0.9708, wrong:[205  86 193  73 385], seed:387\n",
      "roc_auc=0.9978, accuracy=0.9766, wrong:[ 86 193  73 385], seed:820\n",
      "roc_auc=0.9966, accuracy=0.9766, wrong:[205  86  73 385], seed:758\n",
      "roc_auc=0.9939, accuracy=0.9766, wrong:[205  86  73 385], seed:344\n",
      "roc_auc=0.9956, accuracy=0.9766, wrong:[205  86  73 385], seed:593\n",
      "roc_auc=0.9958, accuracy=0.9766, wrong:[205  86  73 385], seed:116\n",
      "MEAN: roc_auc=0.9962, accuracy=0.9754\n",
      "RUNNING 10-times 'train_xgb_native' with {'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "roc_auc=0.9962, accuracy=0.9825, wrong:[ 86  73 385], seed:469\n",
      "roc_auc=0.9966, accuracy=0.9708, wrong:[205 193  73 385  39], seed:452\n",
      "roc_auc=0.9974, accuracy=0.9825, wrong:[205   5  73], seed:552\n",
      "roc_auc=0.9947, accuracy=0.9708, wrong:[205   5  86  73 385], seed:100\n",
      "roc_auc=0.9959, accuracy=0.9766, wrong:[205  86  73 385], seed:832\n",
      "roc_auc=0.9952, accuracy=0.9825, wrong:[  5  73 385], seed:464\n",
      "roc_auc=0.9945, accuracy=0.9708, wrong:[205   5  86  73 385], seed:928\n",
      "roc_auc=0.9963, accuracy=0.9766, wrong:[  5  73 385  39], seed:226\n",
      "roc_auc=0.9921, accuracy=0.9825, wrong:[  5  73 385], seed:984\n",
      "roc_auc=0.9956, accuracy=0.9708, wrong:[205   5  86  73 385], seed:714\n",
      "MEAN: roc_auc=0.9955, accuracy=0.9766\n"
     ]
    }
   ],
   "source": [
    "# using subsample in XGBoost parameters\n",
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'subsample' : 0.8,\n",
    "}\n",
    "\n",
    "for fun in [train_xgb_sklearn, train_xgb_native]:\n",
    "    results = eval_n_times(10, fun, params);\n",
    "    print(\"MEAN: roc_auc={roc:.4f}, accuracy={acc:.4f}\".format(**pd.DataFrame(results).mean().to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def train_xgb_cv_native(X_train, y_train, params, max_rounds, skb):\n",
    "    #params = {**params, 'seed':random_seed}\n",
    "    train = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    result = xgb.cv(params, train, max_rounds, folds=skb, metrics=['error','auc'])\n",
    "    roc, err = result.iloc[-1][['test-auc-mean', 'test-error-mean']].values\n",
    "    return {'roc':roc, 'acc':1-err}\n",
    "\n",
    "def train_xgb_cv_custom(X_train, y_train, params, max_rounds, skb):\n",
    "    fold_results=[]\n",
    "    train = xgb.DMatrix(X_train.values, y_train.values)\n",
    "    for i,s in enumerate(skb.split(X_train,y_train)):\n",
    "        fold_train = train.slice(s[0])\n",
    "        fold_test  = train.slice(s[1])\n",
    "        bst = xgb.train(params, fold_train, max_rounds)\n",
    "        y_hat = bst.predict(fold_test)\n",
    "        metrics = eval_metrics(y_train.iloc[s[1]], y_hat)\n",
    "        fold_results.append(metrics)\n",
    "    return pd.DataFrame(fold_results).mean().to_dict()\n",
    "\n",
    "def eval_n_times_cv(X_train, y_train, n, xgb_cv, params, nfold=5, verbose=True):\n",
    "    print(f\"RUNNING {n}-times {nfold}-fold '{xgb_cv.__name__}' with {params}\")\n",
    "    results = []\n",
    "    start = time()\n",
    "    for i in range(n):\n",
    "        random_seed = np.random.randint(1000)\n",
    "        skb = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=random_seed)\n",
    "        metrics = xgb_cv(X_train, y_train, params, max_rounds, skb)\n",
    "        results.append(metrics)\n",
    "        if (verbose != 0) & (i % verbose == 0):\n",
    "            print(f\"{i}:\", \"roc_auc={roc:.4f}, accuracy={acc:.4f}\".format(**metrics))\n",
    "\n",
    "    print(\"took %.1f seconds\" % (time() - start))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING 5-times 5-fold 'train_xgb_cv_native' with {'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "0: roc_auc=0.9908, accuracy=0.9649\n",
      "1: roc_auc=0.9887, accuracy=0.9597\n",
      "2: roc_auc=0.9887, accuracy=0.9597\n",
      "3: roc_auc=0.9887, accuracy=0.9597\n",
      "4: roc_auc=0.9887, accuracy=0.9597\n",
      "took 4.2 seconds\n",
      "MEAN: roc_auc=0.9891, accuracy=0.9607\n"
     ]
    }
   ],
   "source": [
    "results = eval_n_times_cv(X_train, y_train, 5, train_xgb_cv_native, params, verbose=1)\n",
    "print(\"MEAN: roc_auc={roc:.4f}, accuracy={acc:.4f}\".format(**pd.DataFrame(results).mean().to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING 100-times 5-fold 'train_xgb_cv_custom' with {'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "0: roc_auc=0.9898, accuracy=0.9647\n",
      "10: roc_auc=0.9918, accuracy=0.9723\n",
      "20: roc_auc=0.9918, accuracy=0.9573\n",
      "30: roc_auc=0.9905, accuracy=0.9649\n",
      "40: roc_auc=0.9909, accuracy=0.9598\n",
      "50: roc_auc=0.9891, accuracy=0.9597\n",
      "60: roc_auc=0.9887, accuracy=0.9573\n",
      "70: roc_auc=0.9910, accuracy=0.9723\n",
      "80: roc_auc=0.9911, accuracy=0.9648\n",
      "90: roc_auc=0.9912, accuracy=0.9623\n",
      "took 24.2 seconds\n",
      "MEAN: roc_auc=0.9905, accuracy=0.9628\n"
     ]
    }
   ],
   "source": [
    "results = eval_n_times_cv(X_train, y_train, 100, train_xgb_cv_custom, params, verbose=10)\n",
    "print(\"MEAN: roc_auc={roc:.4f}, accuracy={acc:.4f}\".format(**pd.DataFrame(results).mean().to_dict()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
